{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62859d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Lesson 1-Stripped [Code only].ipynb'  'Lesson 4-Stripped [Code only].ipynb'\n",
      "'Lesson 2-Stripped[Code only].ipynb'   'Lesson 5-Stripped [Code Only].ipynb'\n",
      "'Lesson 3-Stripped [Code only].ipynb'\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b5b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c382ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.nlp import *\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import tqdm as tq\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import collections.abc as collections_abc\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "from torch.autograd import Variable\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "\n",
    "from collections.abc import Iterable, Iterator\n",
    "from timeit import default_timer as timer\n",
    "from abc import abstractmethod\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler, BatchSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2bc3f1-06a9-49cf-955d-8134bcb6f6f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_classes = (str, bytes)\n",
    "\n",
    "\n",
    "def get_tensor(batch, pin, half=False):\n",
    "    if isinstance(batch, (np.ndarray, np.generic)):\n",
    "        batch = T(batch, half=half, cuda=False).contiguous()\n",
    "        if pin: batch = batch.pin_memory()\n",
    "        return to_gpu(batch)\n",
    "    elif isinstance(batch, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(batch, collections_abc.Mapping):\n",
    "        return {k: get_tensor(sample, pin, half) for k, sample in batch.items()}\n",
    "    elif isinstance(batch, collections_abc.Sequence):\n",
    "        return [get_tensor(sample, pin, half) for sample in batch]\n",
    "    raise TypeError(f\"batch must contain numbers, dicts or lists; found {type(batch)}\")\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, pad_idx=0,\n",
    "                 num_workers=None, pin_memory=False, drop_last=False, pre_pad=True, half=False,\n",
    "                 transpose=False, transpose_y=False):\n",
    "        self.dataset,self.batch_size,self.num_workers = dataset,batch_size,num_workers\n",
    "        self.pin_memory,self.drop_last,self.pre_pad = pin_memory,drop_last,pre_pad\n",
    "        self.transpose,self.transpose_y,self.pad_idx,self.half = transpose,transpose_y,pad_idx,half\n",
    "\n",
    "        if batch_sampler is not None:\n",
    "            if batch_size > 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler is mutually exclusive with '\n",
    "                                 'batch_size, shuffle, sampler, and drop_last')\n",
    "\n",
    "        if sampler is not None and shuffle:\n",
    "            raise ValueError('sampler is mutually exclusive with shuffle')\n",
    "\n",
    "        if batch_sampler is None:\n",
    "            if sampler is None:\n",
    "                sampler = RandomSampler(dataset) if shuffle else SequentialSampler(dataset)\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        if num_workers is None:\n",
    "            self.num_workers = num_cpus()\n",
    "\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "\n",
    "    def __len__(self): return len(self.batch_sampler)\n",
    "\n",
    "    def jag_stack(self, b):\n",
    "        if len(b[0].shape) not in (1,2): return np.stack(b)\n",
    "        ml = max(len(o) for o in b)\n",
    "        if min(len(o) for o in b)==ml: return np.stack(b)\n",
    "        res = np.zeros((len(b), ml), dtype=b[0].dtype) + self.pad_idx\n",
    "        for i,o in enumerate(b):\n",
    "            if self.pre_pad: res[i, -len(o):] = o\n",
    "            else:            res[i,  :len(o)] = o\n",
    "        return res\n",
    "\n",
    "    def np_collate(self, batch):\n",
    "        b = batch[0]\n",
    "        if isinstance(b, (np.ndarray, np.generic)): return self.jag_stack(batch)\n",
    "        elif isinstance(b, (int, float)): return np.array(batch)\n",
    "        elif isinstance(b, string_classes): return batch\n",
    "        elif isinstance(b, collections_abc.Mapping):\n",
    "            return {key: self.np_collate([d[key] for d in batch]) for key in b}\n",
    "        elif isinstance(b, collections_abc.Sequence):\n",
    "            return [self.np_collate(samples) for samples in zip(*batch)]\n",
    "        raise TypeError((\"batch must contain numbers, dicts or lists; found {}\".format(type(b))))\n",
    "\n",
    "    def get_batch(self, indices):\n",
    "        res = self.np_collate([self.dataset[i] for i in indices])\n",
    "        if self.transpose:   res[0] = res[0].T\n",
    "        if self.transpose_y: res[1] = res[1].T\n",
    "        return res\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.num_workers==0:\n",
    "            for batch in map(self.get_batch, iter(self.batch_sampler)):\n",
    "                yield get_tensor(batch, self.pin_memory, self.half)\n",
    "        else:\n",
    "            with ThreadPoolExecutor(max_workers=self.num_workers) as e:\n",
    "                # avoid py3.6 issue where queue is infinite and can result in memory exhaustion\n",
    "                for c in chunk_iter(iter(self.batch_sampler), self.num_workers*10):\n",
    "                    for batch in e.map(self.get_batch, c):\n",
    "                        yield get_tensor(batch, self.pin_memory, self.half)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8400658-5d19-4628-b491-8bb2b1867b45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelData():\n",
    "    \"\"\"Encapsulates DataLoaders and Datasets for training, validation, test.\"\"\"\n",
    "    def __init__(self, path, trn_dl, val_dl, test_dl=None):\n",
    "        self.path,self.trn_dl,self.val_dl,self.test_dl = path,trn_dl,val_dl,test_dl\n",
    "\n",
    "    @classmethod\n",
    "    def from_dls(cls, path,trn_dl,val_dl,test_dl=None):\n",
    "        #trn_dl,val_dl = DataLoader(trn_dl),DataLoader(val_dl)\n",
    "        #if test_dl: test_dl = DataLoader(test_dl)\n",
    "        return cls(path, trn_dl, val_dl, test_dl)\n",
    "\n",
    "    @property\n",
    "    def is_reg(self): return self.trn_ds.is_reg\n",
    "    @property\n",
    "    def is_multi(self): return self.trn_ds.is_multi\n",
    "    @property\n",
    "    def trn_ds(self): return self.trn_dl.dataset\n",
    "    @property\n",
    "    def val_ds(self): return self.val_dl.dataset\n",
    "    @property\n",
    "    def test_ds(self): return self.test_dl.dataset\n",
    "    @property\n",
    "    def trn_y(self): return self.trn_ds.y\n",
    "    @property\n",
    "    def val_y(self): return self.val_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed2c65b-86f9-4eca-aaab-44ba1bd3e7cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_train_mode(m):\n",
    "    if (hasattr(m, 'running_mean') and (getattr(m,'bn_freeze',False)\n",
    "              or not getattr(m,'trainable',False))): m.eval()\n",
    "    elif (getattr(m,'drop_freeze',False) and hasattr(m, 'p')\n",
    "          and ('drop' in type(m).__name__.lower())): m.eval()\n",
    "    else: m.train()\n",
    "    \n",
    "def batch_sz(x, seq_first=False):\n",
    "    if is_listy(x): x = x[0]\n",
    "    return x.shape[1 if seq_first else 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33da9f51-a7cf-49f0-ab7c-ff2e56cd372e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stepper():\n",
    "    def __init__(self, m, opt, crit, clip=0, reg_fn=None, fp16=False, loss_scale=1):\n",
    "        self.m,self.opt,self.crit,self.clip,self.reg_fn = m,opt,crit,clip,reg_fn\n",
    "        self.fp16 = fp16\n",
    "        self.reset(True)\n",
    "        if self.fp16: self.fp32_params = copy_model_to_fp32(m, opt)\n",
    "        self.loss_scale = loss_scale\n",
    "\n",
    "    def reset(self, train=True):\n",
    "        if train: apply_leaf(self.m, set_train_mode)\n",
    "        else: self.m.eval()\n",
    "        if hasattr(self.m, 'reset'):\n",
    "            self.m.reset()\n",
    "            if self.fp16: self.fp32_params = copy_model_to_fp32(self.m, self.opt)\n",
    "\n",
    "    def step(self, xs, y, epoch):\n",
    "        xtra = []\n",
    "        output = self.m(*xs)\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        if self.fp16: self.m.zero_grad()\n",
    "        else: self.opt.zero_grad() \n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n",
    "        if self.loss_scale != 1:\n",
    "            for param in self.fp32_params: param.grad.data.div_(self.loss_scale)\n",
    "        if self.clip:   # Gradient clipping\n",
    "            if IS_TORCH_04: nn.utils.clip_grad_norm_(trainable_params_(self.m), self.clip)\n",
    "            else:           nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        if 'wd' in self.opt.param_groups[0] and self.opt.param_groups[0]['wd'] != 0: \n",
    "            #Weight decay out of the loss. After the gradient computation but before the step.\n",
    "            for group in self.opt.param_groups:\n",
    "                lr, wd = group['lr'], group['wd']\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None: p.data = p.data.add(-wd * lr, p.data)\n",
    "        self.opt.step()\n",
    "        if self.fp16: \n",
    "            copy_fp32_to_model(self.m, self.fp32_params)\n",
    "            torch.cuda.synchronize()\n",
    "        return torch_item(raw_loss.data)\n",
    "    \n",
    "    def evaluate(self, xs, y):\n",
    "        preds = self.m(*xs)\n",
    "        if isinstance(preds,tuple): preds=preds[0]\n",
    "        return preds, self.crit(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a0c7fd-4d4c-43a8-a1ce-51bd6f2c1ba9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(stepper, dl, metrics, epoch, seq_first=False, validate_skip = 0):\n",
    "    if epoch < validate_skip: return [float('nan')] + [float('nan')] * len(metrics)\n",
    "    batch_cnts,loss,res = [],[],[]\n",
    "    stepper.reset(False)\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(iter(dl), leave=False, total=len(dl), miniters=0, desc='Validation')\n",
    "        for (*x,y) in t:\n",
    "            y = VV(y)\n",
    "            preds, l = stepper.evaluate(VV(x), y)\n",
    "            batch_cnts.append(batch_sz(x, seq_first=seq_first))\n",
    "            loss.append(to_np(l))\n",
    "            res.append([to_np(f(datafy(preds), datafy(y))) for f in metrics])\n",
    "    return [np.average(loss, 0, weights=batch_cnts)] + list(np.average(np.stack(res), 0, weights=batch_cnts))\n",
    "\n",
    "def append_stats(ep_vals, epoch, values, decimals=6):\n",
    "    ep_vals[epoch]=list(np.round(values, decimals))\n",
    "    return ep_vals\n",
    "\n",
    "def print_stats(epoch, values, visualize, prev_val=[], decimals=6):\n",
    "    layout = \"{!s:^10}\" + \" {!s:10}\" * len(values)\n",
    "    values = [epoch] + list(np.round(values, decimals))\n",
    "    sym = \"\"\n",
    "    if visualize:\n",
    "        if epoch == 0:                                             pass        \n",
    "        elif values[1] > prev_val[0] and values[2] > prev_val[1]:  sym = \" △ △\"\n",
    "        elif values[1] > prev_val[0] and values[2] < prev_val[1]:  sym = \" △ ▼\"            \n",
    "        elif values[1] < prev_val[0] and values[2] > prev_val[1]:  sym = \" ▼ △\"            \n",
    "        elif values[1] < prev_val[0] and values[2] < prev_val[1]:  sym = \" ▼ ▼\"\n",
    "    print(layout.format(*values) + sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94524a3d-3ffa-4cbc-82bb-c984dbf7e345",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(model, data, n_epochs, opt, crit, metrics=None, callbacks=None, stepper=Stepper,\n",
    "        swa_model=None, swa_start=None, swa_eval_freq=None, visualize=False, **kwargs):\n",
    "    \"\"\" Fits a model\n",
    "\n",
    "    Arguments:\n",
    "       model (model): any pytorch module\n",
    "           net = to_gpu(net)\n",
    "       data (ModelData): see ModelData class and subclasses (can be a list)\n",
    "       opts: an optimizer. Example: optim.Adam. \n",
    "       If n_epochs is a list, it needs to be the layer_optimizer to get the optimizer as it changes.\n",
    "       n_epochs(int or list): number of epochs (or list of number of epochs)\n",
    "       crit: loss function to optimize. Example: F.cross_entropy\n",
    "    \"\"\"\n",
    "\n",
    "    seq_first = kwargs.pop('seq_first', False)\n",
    "    all_val = kwargs.pop('all_val', False)\n",
    "    get_ep_vals = kwargs.pop('get_ep_vals', False)\n",
    "    validate_skip = kwargs.pop('validate_skip', 0)\n",
    "    metrics = metrics or []\n",
    "    callbacks = callbacks or []\n",
    "    avg_mom=0.98\n",
    "    batch_num,avg_loss=0,0.\n",
    "    for cb in callbacks: cb.on_train_begin()\n",
    "    names = [\"epoch\", \"trn_loss\", \"val_loss\"] + [f.__name__ for f in metrics]\n",
    "    if swa_model is not None:\n",
    "        swa_names = ['swa_loss'] + [f'swa_{f.__name__}' for f in metrics]\n",
    "        names += swa_names\n",
    "        # will use this to call evaluate later\n",
    "        swa_stepper = stepper(swa_model, None, crit, **kwargs)\n",
    "\n",
    "    layout = \"{!s:10} \" * len(names)\n",
    "    if not isinstance(n_epochs, Iterable): n_epochs=[n_epochs]\n",
    "    if not isinstance(data, Iterable): data = [data]\n",
    "    if len(data) == 1: data = data * len(n_epochs)\n",
    "    for cb in callbacks: cb.on_phase_begin()\n",
    "    model_stepper = stepper(model, opt.opt if hasattr(opt,'opt') else opt, crit, **kwargs)\n",
    "    ep_vals = collections.OrderedDict()\n",
    "    tot_epochs = int(np.ceil(np.array(n_epochs).sum()))\n",
    "    cnt_phases = np.array([ep * len(dat.trn_dl) for (ep,dat) in zip(n_epochs,data)]).cumsum()\n",
    "    phase = 0\n",
    "    for epoch in tnrange(tot_epochs, desc='Epoch'):\n",
    "        if phase >= len(n_epochs): break #Sometimes cumulated errors make this append.\n",
    "        model_stepper.reset(True)\n",
    "        cur_data = data[phase]\n",
    "        if hasattr(cur_data, 'trn_sampler'): cur_data.trn_sampler.set_epoch(epoch)\n",
    "        if hasattr(cur_data, 'val_sampler'): cur_data.val_sampler.set_epoch(epoch)\n",
    "        num_batch = len(cur_data.trn_dl)\n",
    "        t = tqdm(iter(cur_data.trn_dl), leave=False, total=num_batch, miniters=0)\n",
    "        if all_val: val_iter = IterBatch(cur_data.val_dl)\n",
    "\n",
    "        for (*x,y) in t:\n",
    "            batch_num += 1\n",
    "            for cb in callbacks: cb.on_batch_begin()\n",
    "            loss = model_stepper.step(V(x),V(y), epoch)\n",
    "            avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n",
    "            debias_loss = avg_loss / (1 - avg_mom**batch_num)\n",
    "            t.set_postfix(loss=debias_loss, refresh=False)\n",
    "            stop=False\n",
    "            los = debias_loss if not all_val else [debias_loss] + validate_next(model_stepper,metrics, val_iter)\n",
    "            for cb in callbacks: stop = stop or cb.on_batch_end(los)\n",
    "            if stop: return\n",
    "            if batch_num >= cnt_phases[phase]:\n",
    "                for cb in callbacks: cb.on_phase_end()\n",
    "                phase += 1\n",
    "                if phase >= len(n_epochs):\n",
    "                    t.close()\n",
    "                    break\n",
    "                for cb in callbacks: cb.on_phase_begin()\n",
    "                if isinstance(opt, LayerOptimizer): model_stepper.opt = opt.opt\n",
    "                if cur_data != data[phase]:\n",
    "                    t.close()\n",
    "                    break\n",
    "\n",
    "        if not all_val:\n",
    "            vals = validate(model_stepper, cur_data.val_dl, metrics, epoch, seq_first=seq_first, validate_skip = validate_skip)\n",
    "            stop=False\n",
    "            for cb in callbacks: stop = stop or cb.on_epoch_end(vals)\n",
    "            if swa_model is not None:\n",
    "                if (epoch + 1) >= swa_start and ((epoch + 1 - swa_start) % swa_eval_freq == 0 or epoch == tot_epochs - 1):\n",
    "                    fix_batchnorm(swa_model, cur_data.trn_dl)\n",
    "                    swa_vals = validate(swa_stepper, cur_data.val_dl, metrics, epoch, validate_skip = validate_skip)\n",
    "                    vals += swa_vals\n",
    "\n",
    "            if epoch > 0: \n",
    "                print_stats(epoch, [debias_loss] + vals, visualize, prev_val)\n",
    "            else:\n",
    "                print(layout.format(*names))\n",
    "                print_stats(epoch, [debias_loss] + vals, visualize)\n",
    "            prev_val = [debias_loss] + vals\n",
    "            ep_vals = append_stats(ep_vals, epoch, [debias_loss] + vals)\n",
    "        if stop: break\n",
    "    for cb in callbacks: cb.on_train_end()\n",
    "    if get_ep_vals: return vals, ep_vals\n",
    "    else: return vals\n",
    "\n",
    "def torch_item(x): return x.item() if hasattr(x,'item') else x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a79be78-8e21-4442-b1e0-906bde3fb430",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy_multi(preds, targs, thresh):\n",
    "    return ((preds>thresh).float()==targs).float().mean()\n",
    "\n",
    "def sum_geom(a,r,n): return a*n if r==1 else math.ceil(a*(1-r**n)/(1-r))\n",
    "\n",
    "def one_hot(a,c): return np.eye(c)[a]\n",
    "\n",
    "def calc_r(y_i, x, y):\n",
    "    return np.log(calc_pr(y_i, x, y, True) / calc_pr(y_i, x, y, False))\n",
    "\n",
    "def calc_pr(y_i, x, y, b):\n",
    "    idx = np.argwhere((y==y_i)==b)\n",
    "    ct = x[idx[:,0]].sum(0)+1\n",
    "    tot = ((y==y_i)==b).sum()+1\n",
    "    return ct/tot\n",
    "def num_cpus():\n",
    "    try:\n",
    "        return len(os.sched_getaffinity(0))\n",
    "    except AttributeError:\n",
    "        return os.cpu_count()\n",
    "\n",
    "IS_TORCH_04 = LooseVersion(torch.__version__) >= LooseVersion('0.4')\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "def to_gpu(x, *args, **kwargs):\n",
    "    '''puts pytorch variable to gpu, if cuda is available and USE_GPU is set to true. '''\n",
    "    return x.cuda(*args, **kwargs) if USE_GPU else x\n",
    "\n",
    "def T(a, half=False, cuda=True):\n",
    "    \"\"\"\n",
    "    Convert numpy array into a pytorch tensor. \n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(a):\n",
    "        a = np.array(np.ascontiguousarray(a))\n",
    "        if a.dtype in (np.int8, np.int16, np.int32, np.int64):\n",
    "            a = torch.LongTensor(a.astype(np.int64))\n",
    "        elif a.dtype in (np.float32, np.float64):\n",
    "            a = to_half(a) if half else torch.FloatTensor(a)\n",
    "        else: raise NotImplementedError(a.dtype)\n",
    "    if cuda: a = to_gpu(a)\n",
    "    return a\n",
    "\n",
    "def create_variable(x, volatile, requires_grad=False):\n",
    "    if type (x) != Variable:\n",
    "        if IS_TORCH_04: x = Variable(T(x), requires_grad=requires_grad)\n",
    "        else:           x = Variable(T(x), requires_grad=requires_grad, volatile=volatile)\n",
    "    return x\n",
    "\n",
    "def V_(x, requires_grad=False, volatile=False):\n",
    "    '''equivalent to create_variable, which creates a pytorch tensor'''\n",
    "    return create_variable(x, volatile=volatile, requires_grad=requires_grad)\n",
    "\n",
    "def V(x, requires_grad=False, volatile=False):\n",
    "    '''creates a single or a list of pytorch tensors, depending on input x. '''\n",
    "    return map_over(x, lambda o: V_(o, requires_grad, volatile))\n",
    "\n",
    "def VV_(x): \n",
    "    '''creates a volatile tensor, which does not require gradients. '''\n",
    "    return create_variable(x, True)\n",
    "\n",
    "def VV(x):\n",
    "    '''creates a single or a list of pytorch tensors, depending on input x. '''\n",
    "    return map_over(x, VV_)\n",
    "\n",
    "def to_np(v):\n",
    "    '''returns an np.array object given an input of np.array, list, tuple, torch variable or tensor.'''\n",
    "    if isinstance(v, float): return np.array(v)\n",
    "    if isinstance(v, (np.ndarray, np.generic)): return v\n",
    "    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\n",
    "    if isinstance(v, Variable): v=v.data\n",
    "    if torch.cuda.is_available():\n",
    "        if is_half_tensor(v): v=v.float()\n",
    "    if isinstance(v, torch.FloatTensor): v=v.float()\n",
    "    return v.cpu().numpy()\n",
    "\n",
    "def is_half_tensor(v):\n",
    "    return isinstance(v, torch.cuda.HalfTensor)\n",
    "\n",
    "def accuracy_thresh(thresh):\n",
    "    return lambda preds,targs: accuracy_multi(preds, targs, thresh)\n",
    "\n",
    "def children(m): return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def is_listy(x): return isinstance(x, (list,tuple))\n",
    "\n",
    "def map_over(x, f): return [f(o) for o in x] if is_listy(x) else f(x)\n",
    "\n",
    "def is_iter(x): return isinstance(x, collections_abc.Iterable)\n",
    "\n",
    "def listify(x, y):\n",
    "    if not is_iter(x): x=[x]\n",
    "    n = y if type(y)==int else len(y)\n",
    "    if len(x)==1: x = x * n\n",
    "    return x\n",
    "\n",
    "def delistify(x): return x[0] if is_listy(x) else x\n",
    "\n",
    "def datafy(x):\n",
    "    if is_listy(x): return [o.data for o in x]\n",
    "    else:           return x.data\n",
    "\n",
    "def trainable_params_(m):\n",
    "    '''Returns a list of trainable parameters in the model'''\n",
    "    return [p for p in m.parameters() if p.requires_grad]\n",
    "\n",
    "def chain_params(p):\n",
    "    if is_listy(p):\n",
    "        return list(chain(*[trainable_params_(o) for o in p]))\n",
    "    return trainable_params_(p)\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    if len(c)>0:\n",
    "        for l in c: apply_leaf(l,f)\n",
    "        \n",
    "def chunk_iter(iterable, chunk_size):\n",
    "    '''A generator that yields chunks of iterable, chunk_size at a time. '''\n",
    "    while True:\n",
    "        chunk = []\n",
    "        try:\n",
    "            for _ in range(chunk_size): chunk.append(next(iterable))\n",
    "            yield chunk\n",
    "        except StopIteration:\n",
    "            if chunk: yield chunk\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "703695dd-a41a-4659-b114-023fac27995f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BOW_Dataset(Dataset):\n",
    "    def __init__(self, bow, y, max_len):\n",
    "        self.bow,self.max_len = bow,max_len\n",
    "        self.c = int(y.max())+1\n",
    "        self.n,self.vocab_size = bow.shape\n",
    "        self.y = one_hot(y,self.c).astype(np.float32)\n",
    "        x = self.bow.sign()\n",
    "        self.r = np.stack([calc_r(i, x, y).A1 for i in range(self.c)]).T\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.bow.getrow(i)\n",
    "\n",
    "        num_row_entries = row.indices.shape[0]\n",
    "        indices = (row.indices + 1).astype(np.int64)\n",
    "        data = (row.data).astype(np.int64)\n",
    "\n",
    "        if num_row_entries < self.max_len:\n",
    "            # If short, pad\n",
    "            indices = np.pad(indices, (self.max_len - num_row_entries, 0), mode='constant')\n",
    "            data = np.pad(data, (self.max_len - num_row_entries, 0), mode='constant')\n",
    "        else:\n",
    "            # If long, truncate\n",
    "            indices, data = indices[-self.max_len:], data[-self.max_len:]\n",
    "\n",
    "        return indices, data, min(self.max_len, num_row_entries), self.y[i]\n",
    "\n",
    "    def __len__(self): return len(self.bow.indptr)-1\n",
    "\n",
    "\n",
    "class TextClassifierData(ModelData):\n",
    "    @property\n",
    "    def c(self): return self.trn_ds.c\n",
    "\n",
    "    @property\n",
    "    def r(self):\n",
    "        return torch.Tensor(np.concatenate([np.zeros((1,self.c)), self.trn_ds.r]))\n",
    "\n",
    "    def get_model(self, f, **kwargs):\n",
    "        m = to_gpu(f(self.trn_ds.vocab_size, self.c, **kwargs))\n",
    "        m.r.weight.data = to_gpu(self.r)\n",
    "        m.r.weight.requires_grad = False\n",
    "        model = BasicModel(m)\n",
    "        return BOW_Learner(self, model, metrics=[accuracy_thresh(0.5)], opt_fn=optim.Adam)\n",
    "\n",
    "    def dotprod_nb_learner(self, **kwargs): return self.get_model(DotProdNB, **kwargs)\n",
    "    def nb_learner(self, **kwargs): return self.get_model(SimpleNB, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_bow(cls, trn_bow, trn_y, val_bow, val_y, sl):\n",
    "        trn_ds = BOW_Dataset(trn_bow, trn_y, sl)\n",
    "        val_ds = BOW_Dataset(val_bow, val_y, sl)\n",
    "        trn_dl = DataLoader(trn_ds, 64, True)\n",
    "        val_dl = DataLoader(val_ds, 64, False)\n",
    "        return cls('.', trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59b0f31-ab9c-4f0d-9e27-a1b44712b168",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, models, opt_fn=None, tmp_name='tmp', models_name='models', metrics=None, clip=None, crit=None):\n",
    "        \"\"\"\n",
    "        Combines a ModelData object with a nn.Module object, such that you can train that\n",
    "        module.\n",
    "        data (ModelData): An instance of ModelData.\n",
    "        models(module): chosen neural architecture for solving a supported problem.\n",
    "        opt_fn(function): optimizer function, uses SGD with Momentum of .9 if none.\n",
    "        tmp_name(str): output name of the directory containing temporary files from training process\n",
    "        models_name(str): output name of the directory containing the trained model\n",
    "        metrics(list): array of functions for evaluating a desired metric. Eg. accuracy.\n",
    "        clip(float): gradient clip chosen to limit the change in the gradient to prevent exploding gradients Eg. .3\n",
    "        \"\"\"\n",
    "        self.data_,self.models,self.metrics,self.clip = data,models,metrics,clip\n",
    "        self.sched=None\n",
    "        self.wd_sched = None\n",
    "        self.opt_fn = opt_fn or SGD_Momentum(0.9)\n",
    "        self.tmp_path = tmp_name if os.path.isabs(tmp_name) else os.path.join(self.data.path, tmp_name)\n",
    "        self.models_path = models_name if os.path.isabs(models_name) else os.path.join(self.data.path, models_name)\n",
    "        os.makedirs(self.tmp_path, exist_ok=True)\n",
    "        os.makedirs(self.models_path, exist_ok=True)\n",
    "        self.crit = crit if crit else self._get_crit(data)\n",
    "        self.reg_fn = None\n",
    "        self.fp16 = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_data(cls, m, data, **kwargs):\n",
    "        self = cls(data, BasicModel(to_gpu(m)), **kwargs)\n",
    "        self.unfreeze()\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self,i): return self.children[i]\n",
    "\n",
    "    @property\n",
    "    def children(self): return children(self.model)\n",
    "\n",
    "    @property\n",
    "    def model(self): return self.models.model\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.data_\n",
    "\n",
    "    def summary(self): return model_summary(self.model, [torch.rand(3, 3, self.data.sz,self.data.sz)])\n",
    "\n",
    "    def __repr__(self): return self.model.__repr__()\n",
    "    \n",
    "    def lsuv_init(self, needed_std=1.0, std_tol=0.1, max_attempts=10, do_orthonorm=False):         \n",
    "        x = V(next(iter(self.data.trn_dl))[0])\n",
    "        self.models.model=apply_lsuv_init(self.model, x, needed_std=needed_std, std_tol=std_tol,\n",
    "                            max_attempts=max_attempts, do_orthonorm=do_orthonorm, \n",
    "                            cuda=USE_GPU and torch.cuda.is_available())\n",
    "\n",
    "    def set_bn_freeze(self, m, do_freeze):\n",
    "        if hasattr(m, 'running_mean'): m.bn_freeze = do_freeze\n",
    "\n",
    "    def bn_freeze(self, do_freeze):\n",
    "        apply_leaf(self.model, lambda m: self.set_bn_freeze(m, do_freeze))\n",
    "\n",
    "    def freeze_to(self, n):\n",
    "        c=self.get_layer_groups()\n",
    "        for l in c:     set_trainable(l, False)\n",
    "        for l in c[n:]: set_trainable(l, True)\n",
    "\n",
    "    def freeze_all_but(self, n):\n",
    "        c=self.get_layer_groups()\n",
    "        for l in c: set_trainable(l, False)\n",
    "        set_trainable(c[n], True)\n",
    "        \n",
    "    def freeze_groups(self, groups):\n",
    "        c = self.get_layer_groups()\n",
    "        self.unfreeze()\n",
    "        for g in groups:\n",
    "            set_trainable(c[g], False)\n",
    "            \n",
    "    def unfreeze_groups(self, groups):\n",
    "        c = self.get_layer_groups()\n",
    "        for g in groups:\n",
    "            set_trainable(c[g], True)\n",
    "\n",
    "    def unfreeze(self): self.freeze_to(0)\n",
    "\n",
    "    def get_model_path(self, name): return os.path.join(self.models_path,name)+'.h5'\n",
    "    \n",
    "    def save(self, name): \n",
    "        save_model(self.model, self.get_model_path(name))\n",
    "        if hasattr(self, 'swa_model'): save_model(self.swa_model, self.get_model_path(name)[:-3]+'-swa.h5')\n",
    "                       \n",
    "    def load(self, name): \n",
    "        load_model(self.model, self.get_model_path(name))\n",
    "        if hasattr(self, 'swa_model'): load_model(self.swa_model, self.get_model_path(name)[:-3]+'-swa.h5')\n",
    "\n",
    "    def set_data(self, data): self.data_ = data\n",
    "\n",
    "    def get_cycle_end(self, name):\n",
    "        if name is None: return None\n",
    "        return lambda sched, cycle: self.save_cycle(name, cycle)\n",
    "\n",
    "    def save_cycle(self, name, cycle): self.save(f'{name}_cyc_{cycle}')\n",
    "    def load_cycle(self, name, cycle): self.load(f'{name}_cyc_{cycle}')\n",
    "\n",
    "    def half(self):\n",
    "        if self.fp16: return\n",
    "        self.fp16 = True\n",
    "        if type(self.model) != FP16: self.models.model = FP16(self.model)\n",
    "    def float(self):\n",
    "        if not self.fp16: return\n",
    "        self.fp16 = False\n",
    "        if type(self.model) == FP16: self.models.model = self.model.module\n",
    "        self.model.float()\n",
    "\n",
    "    def fit_gen(self, model, data, layer_opt, n_cycle, cycle_len=None, cycle_mult=1, cycle_save_name=None, best_save_name=None,\n",
    "                use_clr=None, use_clr_beta=None, metrics=None, callbacks=None, use_wd_sched=False, norm_wds=False,             \n",
    "                wds_sched_mult=None, use_swa=False, swa_start=1, swa_eval_freq=5, **kwargs):\n",
    "\n",
    "        if cycle_save_name:\n",
    "            assert use_clr or use_clr_beta or cycle_len, \"cycle_save_name argument requires either of the following arguments use_clr, use_clr_beta, cycle_len\"\n",
    "\n",
    "        if callbacks is None: callbacks=[]\n",
    "        if metrics is None: metrics=self.metrics\n",
    "\n",
    "        if use_wd_sched:\n",
    "            # This needs to come before CosAnneal() because we need to read the initial learning rate from\n",
    "            # layer_opt.lrs - but CosAnneal() alters the layer_opt.lrs value initially (divides by 100)\n",
    "            if np.sum(layer_opt.wds) == 0:\n",
    "                print('fit() warning: use_wd_sched is set to True, but weight decay(s) passed are 0. Use wds to '\n",
    "                      'pass weight decay values.')\n",
    "            batch_per_epoch = len(data.trn_dl)\n",
    "            cl = cycle_len if cycle_len else 1\n",
    "            self.wd_sched = WeightDecaySchedule(layer_opt, batch_per_epoch, cl, cycle_mult, n_cycle,\n",
    "                                                norm_wds, wds_sched_mult)\n",
    "            callbacks += [self.wd_sched]\n",
    "\n",
    "        if use_clr is not None:\n",
    "            clr_div,cut_div = use_clr[:2]\n",
    "            moms = use_clr[2:] if len(use_clr) > 2 else None\n",
    "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
    "            assert cycle_len, \"use_clr requires cycle_len arg\"\n",
    "            self.sched = CircularLR(layer_opt, len(data.trn_dl)*cycle_len, on_cycle_end=cycle_end, div=clr_div, cut_div=cut_div,\n",
    "                                    momentums=moms)\n",
    "        elif use_clr_beta is not None:\n",
    "            div,pct = use_clr_beta[:2]\n",
    "            moms = use_clr_beta[2:] if len(use_clr_beta) > 3 else None\n",
    "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
    "            assert cycle_len, \"use_clr_beta requires cycle_len arg\"\n",
    "            self.sched = CircularLR_beta(layer_opt, len(data.trn_dl)*cycle_len, on_cycle_end=cycle_end, div=div,\n",
    "                                    pct=pct, momentums=moms)\n",
    "        elif cycle_len:\n",
    "            cycle_end = self.get_cycle_end(cycle_save_name)\n",
    "            cycle_batches = len(data.trn_dl)*cycle_len\n",
    "            self.sched = CosAnneal(layer_opt, cycle_batches, on_cycle_end=cycle_end, cycle_mult=cycle_mult)\n",
    "        elif not self.sched: self.sched=LossRecorder(layer_opt)\n",
    "        callbacks+=[self.sched]\n",
    "\n",
    "        if best_save_name is not None:\n",
    "            callbacks+=[SaveBestModel(self, layer_opt, metrics, best_save_name)]\n",
    "\n",
    "        if use_swa:\n",
    "            # make a copy of the model to track average weights\n",
    "            self.swa_model = copy.deepcopy(model)\n",
    "            callbacks+=[SWA(model, self.swa_model, swa_start)]\n",
    "\n",
    "        n_epoch = int(sum_geom(cycle_len if cycle_len else 1, cycle_mult, n_cycle))\n",
    "        return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n",
    "            metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n",
    "            swa_model=self.swa_model if use_swa else None, swa_start=swa_start, \n",
    "            swa_eval_freq=swa_eval_freq, **kwargs)\n",
    "\n",
    "    def get_layer_groups(self): return self.models.get_layer_groups()\n",
    "\n",
    "    def get_layer_opt(self, lrs, wds):\n",
    "        return LayerOptimizer(self.opt_fn, self.get_layer_groups(), lrs, wds)\n",
    "\n",
    "    def fit(self, lrs, n_cycle, wds=None, **kwargs):\n",
    "        self.sched = None\n",
    "        layer_opt = self.get_layer_opt(lrs, wds)\n",
    "        return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n",
    "\n",
    "    def warm_up(self, lr, wds=None):\n",
    "        layer_opt = self.get_layer_opt(lr/4, wds)\n",
    "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), lr, linear=True)\n",
    "        return self.fit_gen(self.model, self.data, layer_opt, 1)\n",
    "\n",
    "    def lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\n",
    "        self.save('tmp')\n",
    "        layer_opt = self.get_layer_opt(start_lr, wds)\n",
    "        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n",
    "        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n",
    "        self.load('tmp')\n",
    "\n",
    "    def lr_find2(self, start_lr=1e-5, end_lr=10, num_it = 100, wds=None, linear=False, stop_dv=True, **kwargs):\n",
    "        self.save('tmp')\n",
    "        layer_opt = self.get_layer_opt(start_lr, wds)\n",
    "        self.sched = LR_Finder2(layer_opt, num_it, end_lr, linear=linear, metrics=self.metrics, stop_dv=stop_dv)\n",
    "        self.fit_gen(self.model, self.data, layer_opt, num_it//len(self.data.trn_dl) + 1, all_val=True, **kwargs)\n",
    "        self.load('tmp')\n",
    "\n",
    "    def predict(self, is_test=False, use_swa=False):\n",
    "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
    "        m = self.swa_model if use_swa else self.model\n",
    "        return predict(m, dl)\n",
    "\n",
    "    def predict_with_targs(self, is_test=False, use_swa=False):\n",
    "        dl = self.data.test_dl if is_test else self.data.val_dl\n",
    "        m = self.swa_model if use_swa else self.model\n",
    "        return predict_with_targs(m, dl)\n",
    "\n",
    "    def predict_dl(self, dl): return predict_with_targs(self.model, dl)[0]\n",
    "\n",
    "    def predict_array(self, arr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            arr: a numpy array to be used as input to the model for prediction purposes\n",
    "        Returns:\n",
    "            a numpy array containing the predictions from the model\n",
    "        \"\"\"\n",
    "        if not isinstance(arr, np.ndarray): raise OSError(f'Not valid numpy array')\n",
    "        self.model.eval()\n",
    "        return to_np(self.model(to_gpu(V(T(arr)))))\n",
    "\n",
    "    def TTA(self, n_aug=4, is_test=False):\n",
    "        \"\"\" Predict with Test Time Augmentation (TTA)\n",
    "\n",
    "        Additional to the original test/validation images, apply image augmentation to them\n",
    "        (just like for training images) and calculate the mean of predictions. The intent\n",
    "        is to increase the accuracy of predictions by examining the images using multiple\n",
    "        perspectives.\n",
    "\n",
    "\n",
    "            n_aug: a number of augmentation images to use per original image\n",
    "            is_test: indicate to use test images; otherwise use validation images\n",
    "\n",
    "        Returns:\n",
    "            (tuple): a tuple containing:\n",
    "\n",
    "                log predictions (numpy.ndarray): log predictions (i.e. `np.exp(log_preds)` will return probabilities)\n",
    "                targs (numpy.ndarray): target values when `is_test==False`; zeros otherwise.\n",
    "        \"\"\"\n",
    "        dl1 = self.data.test_dl     if is_test else self.data.val_dl\n",
    "        dl2 = self.data.test_aug_dl if is_test else self.data.aug_dl\n",
    "        preds1,targs = predict_with_targs(self.model, dl1)\n",
    "        preds1 = [preds1]*math.ceil(n_aug/4)\n",
    "        preds2 = [predict_with_targs(self.model, dl2)[0] for i in tqdm(range(n_aug), leave=False)]\n",
    "        return np.stack(preds1+preds2), targs\n",
    "\n",
    "    def fit_opt_sched(self, phases, cycle_save_name=None, best_save_name=None, stop_div=False, data_list=None, callbacks=None, \n",
    "                      cut = None, use_swa=False, swa_start=1, swa_eval_freq=5, **kwargs):\n",
    "        if data_list is None: data_list=[]\n",
    "        if callbacks is None: callbacks=[]\n",
    "        layer_opt = LayerOptimizer(phases[0].opt_fn, self.get_layer_groups(), 1e-2, phases[0].wds)\n",
    "        if len(data_list) == 0: nb_batches = [len(self.data.trn_dl)] * len(phases)\n",
    "        else: nb_batches = [len(data.trn_dl) for data in data_list] \n",
    "        self.sched = OptimScheduler(layer_opt, phases, nb_batches, stop_div)\n",
    "        callbacks.append(self.sched)\n",
    "        metrics = self.metrics\n",
    "        if best_save_name is not None:\n",
    "            callbacks+=[SaveBestModel(self, layer_opt, metrics, best_save_name)]\n",
    "        if use_swa:\n",
    "            # make a copy of the model to track average weights\n",
    "            self.swa_model = copy.deepcopy(self.model)\n",
    "            callbacks+=[SWA(self.model, self.swa_model, swa_start)]\n",
    "        n_epochs = [phase.epochs for phase in phases] if cut is None else cut\n",
    "        if len(data_list)==0: data_list = [self.data]\n",
    "        return fit(self.model, data_list, n_epochs,layer_opt, self.crit,\n",
    "            metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n",
    "            swa_model=self.swa_model if use_swa else None, swa_start=swa_start, \n",
    "            swa_eval_freq=swa_eval_freq, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d8279d-cfc0-4076-8ea8-216199aee1bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicModel():\n",
    "    def __init__(self,model,name='unnamed'): self.model,self.name = model,name\n",
    "    def get_layer_groups(self, do_fc=False): return children(self.model)\n",
    "\n",
    "class SingleModel(BasicModel):\n",
    "    def get_layer_groups(self): return [self.model]\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for l in self.layers:\n",
    "            l_x = l(x)\n",
    "            x = F.relu(l_x)\n",
    "        return F.log_softmax(l_x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8866bad-984f-44b6-b4d3-095c35aed50a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def opt_params(parm, lr, wd):\n",
    "    return {'params': chain_params(parm), 'lr':lr, 'weight_decay':wd}\n",
    "\n",
    "class LayerOptimizer():\n",
    "    def __init__(self, opt_fn, layer_groups, lrs, wds=None):\n",
    "        if not isinstance(layer_groups, (list,tuple)): layer_groups=[layer_groups]\n",
    "        lrs = listify(lrs, layer_groups)\n",
    "        if wds is None: wds=0.\n",
    "        wds = listify(wds, layer_groups)\n",
    "        self.layer_groups,self.lrs,self.wds = layer_groups,lrs,wds\n",
    "        self.opt = opt_fn(self.opt_params())\n",
    "\n",
    "    def opt_params(self):\n",
    "        assert len(self.layer_groups) == len(self.lrs), f'size mismatch, expected {len(self.layer_groups)} lrs, but got {len(self.lrs)}'\n",
    "        assert len(self.layer_groups) == len(self.wds), f'size mismatch, expected {len(self.layer_groups)} wds, but got {len(self.wds)}'\n",
    "        params = list(zip(self.layer_groups,self.lrs,self.wds))\n",
    "        return [opt_params(*p) for p in params]\n",
    "\n",
    "    @property\n",
    "    def lr(self): return self.lrs[-1]\n",
    "\n",
    "    @property\n",
    "    def mom(self):\n",
    "        if 'betas' in self.opt.param_groups[0]:\n",
    "            return self.opt.param_groups[0]['betas'][0]\n",
    "        else:\n",
    "            return self.opt.param_groups[0]['momentum']\n",
    "\n",
    "    def set_lrs(self, lrs):\n",
    "        lrs = listify(lrs, self.layer_groups)\n",
    "        set_lrs(self.opt, lrs)\n",
    "        self.lrs=lrs\n",
    "\n",
    "    def set_wds_out(self, wds):\n",
    "        wds = listify(wds, self.layer_groups)\n",
    "        set_wds_out(self.opt, wds)\n",
    "        set_wds(self.opt, [0] * len(self.layer_groups))\n",
    "        self.wds=wds\n",
    "\n",
    "    def set_wds(self, wds):\n",
    "        wds = listify(wds, self.layer_groups)\n",
    "        set_wds(self.opt, wds)\n",
    "        set_wds_out(self.opt, [0] * len(self.layer_groups))\n",
    "        self.wds=wds\n",
    "    \n",
    "    def set_mom(self,momentum):\n",
    "        if 'betas' in self.opt.param_groups[0]:\n",
    "            for pg in self.opt.param_groups: pg['betas'] = (momentum, pg['betas'][1])\n",
    "        else:\n",
    "            for pg in self.opt.param_groups: pg['momentum'] = momentum\n",
    "    \n",
    "    def set_beta(self,beta):\n",
    "        if 'betas' in self.opt.param_groups[0]:\n",
    "            for pg in self.opt.param_groups: pg['betas'] = (pg['betas'][0],beta)\n",
    "        elif 'alpha' in self.opt.param_groups[0]:\n",
    "            for pg in self.opt.param_groups: pg['alpha'] = beta\n",
    "\n",
    "    def set_opt_fn(self, opt_fn):\n",
    "        if type(self.opt) != type(opt_fn(self.opt_params())):\n",
    "            self.opt = opt_fn(self.opt_params())\n",
    "\n",
    "def zip_strict_(l, r):\n",
    "    assert len(l) == len(r), f'size mismatch, expected lengths {len(l)}, but got {len(l)} and {len(r)} instead.'\n",
    "    return zip(l, r)\n",
    "\n",
    "def set_lrs(opt, lrs):\n",
    "    lrs = listify(lrs, opt.param_groups)\n",
    "    for pg,lr in zip_strict_(opt.param_groups,lrs): pg['lr'] = lr\n",
    "\n",
    "def set_wds_out(opt, wds):\n",
    "    wds = listify(wds, opt.param_groups)\n",
    "    for pg,wd in zip_strict_(opt.param_groups,wds): pg['wd'] = wd\n",
    "\n",
    "def set_wds(opt, wds):\n",
    "    wds = listify(wds, opt.param_groups)\n",
    "    for pg,wd in zip_strict_(opt.param_groups,wds): pg['weight_decay'] = wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5013491c-3f3a-46d5-b0c8-e943d5148c6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BOW_Learner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c8fa38-fe8d-49c9-864d-0583f361ca6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def on_train_begin(self): pass\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics): pass\n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, metrics): pass\n",
    "    def on_train_end(self): pass\n",
    "\n",
    "class LoggingCallback(Callback):\n",
    "    '''\n",
    "    A class useful for maintaining status of a long-running job.\n",
    "    e.g.: learn.fit(0.01, 1, callbacks = [LoggingCallback(save_path=\"/tmp/log\")])\n",
    "    '''\n",
    "    def __init__(self, save_path):\n",
    "        super().__init__()\n",
    "        self.save_path=save_path\n",
    "    def on_train_begin(self):\n",
    "        self.batch = 0\n",
    "        self.epoch = 0\n",
    "        self.phase = 0\n",
    "        self.f = open(self.save_path, \"a\", 1)\n",
    "        self.log(\"\\ton_train_begin\")\n",
    "    def on_batch_begin(self):\n",
    "        self.log(str(self.batch)+\"\\ton_batch_begin\")\n",
    "    def on_phase_begin(self):\n",
    "        self.log(str(self.phase)+\"\\ton_phase_begin\")\n",
    "    def on_epoch_end(self, metrics):\n",
    "        self.log(str(self.epoch)+\"\\ton_epoch_end: \"+str(metrics))\n",
    "        self.epoch += 1\n",
    "    def on_phase_end(self):\n",
    "        self.log(str(self.phase)+\"\\ton_phase_end\")\n",
    "        self.phase+=1\n",
    "    def on_batch_end(self, metrics):\n",
    "        self.log(str(self.batch)+\"\\ton_batch_end: \"+str(metrics))\n",
    "        self.batch += 1\n",
    "    def on_train_end(self):\n",
    "        self.log(\"\\ton_train_end\")\n",
    "        self.f.close()\n",
    "    def log(self, string):\n",
    "        self.f.write(time.strftime(\"%Y-%m-%dT%H:%M:%S\")+\"\\t\"+string+\"\\n\")\n",
    "        \n",
    "class LossRecorder(Callback):\n",
    "    '''\n",
    "    Saves and displays loss functions and other metrics. \n",
    "    Default sched when none is specified in a learner. \n",
    "    '''\n",
    "    def __init__(self, layer_opt, save_path='', record_mom=False, metrics=[]):\n",
    "        super().__init__()\n",
    "        self.layer_opt=layer_opt\n",
    "        self.init_lrs=np.array(layer_opt.lrs)\n",
    "        self.save_path, self.record_mom, self.metrics = save_path, record_mom, metrics\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        self.losses,self.lrs,self.iterations,self.epochs,self.times = [],[],[],[],[]\n",
    "        self.start_at = timer()\n",
    "        self.val_losses, self.rec_metrics = [], []\n",
    "        if self.record_mom:\n",
    "            self.momentums = []\n",
    "        self.iteration = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, metrics):\n",
    "        self.epoch += 1\n",
    "        self.epochs.append(self.iteration)\n",
    "        self.times.append(timer() - self.start_at)\n",
    "        self.save_metrics(metrics)\n",
    "\n",
    "    def on_batch_end(self, loss):\n",
    "        self.iteration += 1\n",
    "        self.lrs.append(self.layer_opt.lr)\n",
    "        self.iterations.append(self.iteration)\n",
    "        if isinstance(loss, list):\n",
    "            self.losses.append(loss[0])\n",
    "            self.save_metrics(loss[1:])\n",
    "        else: self.losses.append(loss)\n",
    "        if self.record_mom: self.momentums.append(self.layer_opt.mom)\n",
    "\n",
    "    def save_metrics(self,vals):\n",
    "        self.val_losses.append(delistify(vals[0]))\n",
    "        if len(vals) > 2: self.rec_metrics.append(vals[1:])\n",
    "        elif len(vals) == 2: self.rec_metrics.append(vals[1])\n",
    "\n",
    "    def plot_loss(self, n_skip=10, n_skip_end=5):\n",
    "        '''\n",
    "        plots loss function as function of iterations. \n",
    "        When used in Jupyternotebook, plot will be displayed in notebook. Else, plot will be displayed in console and both plot and loss are saved in save_path. \n",
    "        '''\n",
    "        if not in_ipynb(): plt.switch_backend('agg')\n",
    "        plt.plot(self.iterations[n_skip:-n_skip_end], self.losses[n_skip:-n_skip_end])\n",
    "        if not in_ipynb():\n",
    "            plt.savefig(os.path.join(self.save_path, 'loss_plot.png'))\n",
    "            np.save(os.path.join(self.save_path, 'losses.npy'), self.losses[10:])\n",
    "\n",
    "    def plot_lr(self):\n",
    "        '''Plots learning rate in jupyter notebook or console, depending on the enviroment of the learner.'''\n",
    "        if not in_ipynb():\n",
    "            plt.switch_backend('agg')\n",
    "        if self.record_mom:\n",
    "            fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
    "            for i in range(0,2): axs[i].set_xlabel('iterations')\n",
    "            axs[0].set_ylabel('learning rate')\n",
    "            axs[1].set_ylabel('momentum')\n",
    "            axs[0].plot(self.iterations,self.lrs)\n",
    "            axs[1].plot(self.iterations,self.momentums)   \n",
    "        else:\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"learning rate\")\n",
    "            plt.plot(self.iterations, self.lrs)\n",
    "        if not in_ipynb():\n",
    "            plt.savefig(os.path.join(self.save_path, 'lr_plot.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba32d64b-1a21-420b-b9bd-94f02fe7e660",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LR_Updater(LossRecorder):\n",
    "    '''\n",
    "    Abstract class where all Learning Rate updaters inherit from. (e.g., CirularLR)\n",
    "    Calculates and updates new learning rate and momentum at the end of each batch. \n",
    "    Have to be extended. \n",
    "    '''\n",
    "    def on_train_begin(self):\n",
    "        super().on_train_begin()\n",
    "        self.update_lr()\n",
    "        if self.record_mom:\n",
    "            self.update_mom()\n",
    "\n",
    "    def on_batch_end(self, loss):\n",
    "        res = super().on_batch_end(loss)\n",
    "        self.update_lr()\n",
    "        if self.record_mom:\n",
    "            self.update_mom()\n",
    "        return res\n",
    "\n",
    "    def update_lr(self):\n",
    "        new_lrs = self.calc_lr(self.init_lrs)\n",
    "        self.layer_opt.set_lrs(new_lrs)\n",
    "    \n",
    "    def update_mom(self):\n",
    "        new_mom = self.calc_mom()\n",
    "        self.layer_opt.set_mom(new_mom)\n",
    "\n",
    "    @abstractmethod\n",
    "    def calc_lr(self, init_lrs): raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def calc_mom(self): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8b6b83-01e5-4b0f-9d64-b31474eb8c23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CosAnneal(LR_Updater):\n",
    "    ''' Learning rate scheduler that implements a cosine annealation schedule. '''\n",
    "    def __init__(self, layer_opt, nb, on_cycle_end=None, cycle_mult=1):\n",
    "        self.nb,self.on_cycle_end,self.cycle_mult = nb,on_cycle_end,cycle_mult\n",
    "        super().__init__(layer_opt)\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        self.cycle_iter,self.cycle_count=0,0\n",
    "        super().on_train_begin()\n",
    "\n",
    "    def calc_lr(self, init_lrs):\n",
    "        if self.iteration<self.nb/20:\n",
    "            self.cycle_iter += 1\n",
    "            return init_lrs/100.\n",
    "\n",
    "        cos_out = np.cos(np.pi*(self.cycle_iter)/self.nb) + 1\n",
    "        self.cycle_iter += 1\n",
    "        if self.cycle_iter==self.nb:\n",
    "            self.cycle_iter = 0\n",
    "            self.nb *= self.cycle_mult\n",
    "            if self.on_cycle_end: self.on_cycle_end(self, self.cycle_count)\n",
    "            self.cycle_count += 1\n",
    "        return init_lrs / 2 * cos_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f15c95-6191-4e27-8935-6085f8aff40e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DotProdNB(nn.Module):\n",
    "    def __init__(self, nf, ny, w_adj=0.4, r_adj=10):\n",
    "        super().__init__()\n",
    "        self.w_adj,self.r_adj = w_adj,r_adj\n",
    "        self.w = nn.Embedding(nf+1, 1, padding_idx=0)\n",
    "        self.w.weight.data.uniform_(-0.1,0.1)\n",
    "        self.r = nn.Embedding(nf+1, ny)\n",
    "\n",
    "    def forward(self, feat_idx, feat_cnt, sz):\n",
    "        w = self.w(feat_idx)\n",
    "        r = self.r(feat_idx)\n",
    "        x = ((w+self.w_adj)*r/self.r_adj).sum(1)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e15f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = \"../../nb5_data/aclImdb/\"\n",
    "names = ['neg','pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e58dd46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def texts_labels_from_folders(path, folders):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(folders):\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(idx)\n",
    "    return texts, np.array(labels).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673c64e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\n",
    "val,val_y = texts_labels_from_folders(f'{PATH}test',names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9fa0433",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A retired diplomat, played nicely by Michael York, goes to Russia to get revenge on the Russian gangster that murdered the diplomat\\'s policeman son. There the diplomat meets an exceptionally strong and decent Russian cop who helps him bring the Russian gangster to justice.<br /><br />I remembered the old action flicks of the 1980s that always portray the Russians as evil bad guys out to undermine the righteous U.S. government. It\\'s interesting to see this time the Russian guy as a hero.<br /><br />Not a great flick, it\\'s really typically a \"B\" action flick. Michael York lends some class to this mediocre movie. Alexander Nevsky, who plays the Russian cop is kind of \"blah\" but surprisingly has some chemistry with Michael York. Face it, Michael York is such a good actor that he\\'d have chemistry with anyone he\\'s doing a scene with. Disappointingly, the handsome Adrian Paul gets killed within the first 15 minutes into the movie. Now, if Adrian Paul was in this movie longer, it would\\'ve been an above average \"B\" action flick. All I can say about Adrian Paul is that he is real nice to look at for the first 15 minutes of the movie. The villain, played by Richard Tyson, is your typical bad guy. He\\'s very blonde and very villainous in this movie.<br /><br />Rent this flick if there is nothing else on TV to watch. It\\'s okay. It doesn\\'t suck too bad. The action scenes are decent. The acting could be better, the plot could\\'ve moved much faster, but hey, you get to see what Russia looks like today!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3893a969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077ca8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "re_tok = re.compile('([!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~“”¨«»®´·º½¾¿¡§£₤‘’])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e66f833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a8f7cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "veczr = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f1a0c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1ef5a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3749745 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b998e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 166 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8945fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aussie', 'aussies', 'austen', 'austeniana', 'austens']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = veczr.get_feature_names(); vocab[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d5cb649",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"b\"',\n",
       " '\"blah\"',\n",
       " '/><br',\n",
       " '/>i',\n",
       " '/>not',\n",
       " '/>rent',\n",
       " '15',\n",
       " '1980s',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'adrian',\n",
       " 'alexander',\n",
       " 'all',\n",
       " 'always',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anyone',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'average',\n",
       " 'bad',\n",
       " 'bad.',\n",
       " 'be',\n",
       " 'been',\n",
       " 'better,',\n",
       " 'blonde',\n",
       " 'bring',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'chemistry',\n",
       " 'class',\n",
       " 'cop',\n",
       " 'could',\n",
       " \"could've\",\n",
       " 'decent',\n",
       " 'decent.',\n",
       " 'diplomat',\n",
       " \"diplomat's\",\n",
       " 'diplomat,',\n",
       " 'disappointingly,',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'else',\n",
       " 'evil',\n",
       " 'exceptionally',\n",
       " 'face',\n",
       " 'faster,',\n",
       " 'first',\n",
       " 'flick',\n",
       " 'flick,',\n",
       " 'flick.',\n",
       " 'flicks',\n",
       " 'for',\n",
       " 'gangster',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'goes',\n",
       " 'good',\n",
       " 'government.',\n",
       " 'great',\n",
       " 'guy',\n",
       " 'guy.',\n",
       " 'guys',\n",
       " 'handsome',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'helps',\n",
       " 'hero.<br',\n",
       " 'hey,',\n",
       " 'him',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'interesting',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'it,',\n",
       " 'justice.<br',\n",
       " 'killed',\n",
       " 'kind',\n",
       " 'lends',\n",
       " 'like',\n",
       " 'longer,',\n",
       " 'look',\n",
       " 'looks',\n",
       " 'mediocre',\n",
       " 'meets',\n",
       " 'michael',\n",
       " 'minutes',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'movie.',\n",
       " 'movie.<br',\n",
       " 'much',\n",
       " 'murdered',\n",
       " 'nevsky,',\n",
       " 'nice',\n",
       " 'nicely',\n",
       " 'nothing',\n",
       " 'now,',\n",
       " 'of',\n",
       " 'okay.',\n",
       " 'old',\n",
       " 'on',\n",
       " 'out',\n",
       " 'paul',\n",
       " 'played',\n",
       " 'plays',\n",
       " 'plot',\n",
       " 'policeman',\n",
       " 'portray',\n",
       " 'real',\n",
       " 'really',\n",
       " 'remembered',\n",
       " 'retired',\n",
       " 'revenge',\n",
       " 'richard',\n",
       " 'righteous',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'russians',\n",
       " 'say',\n",
       " 'scene',\n",
       " 'scenes',\n",
       " 'see',\n",
       " 'some',\n",
       " 'son.',\n",
       " 'strong',\n",
       " 'such',\n",
       " 'suck',\n",
       " 'surprisingly',\n",
       " 'that',\n",
       " 'the',\n",
       " 'there',\n",
       " 'this',\n",
       " 'time',\n",
       " 'to',\n",
       " 'today!',\n",
       " 'too',\n",
       " 'tv',\n",
       " 'typical',\n",
       " 'typically',\n",
       " 'tyson,',\n",
       " 'u.s.',\n",
       " 'undermine',\n",
       " 'very',\n",
       " 'villain,',\n",
       " 'villainous',\n",
       " 'was',\n",
       " 'watch.',\n",
       " 'what',\n",
       " 'who',\n",
       " 'with',\n",
       " 'with.',\n",
       " 'within',\n",
       " \"would've\",\n",
       " 'york',\n",
       " 'york,',\n",
       " 'york.',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = set([o.lower() for o in trn[0].split(' ')]); w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a77fbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0865acfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veczr.vocabulary_['absurd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d874b24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,1297]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f5ea30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc[0,5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f23f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a04c852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=trn_term_doc\n",
    "y=trn_y\n",
    "\n",
    "r = np.log(pr(1)/pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba48c809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81656"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_preds = val_term_doc @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a08488c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83016"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=trn_term_doc.sign()\n",
    "r = np.log(pr(1)/pr(0))\n",
    "\n",
    "pre_preds = val_term_doc.sign() @ r.T + b\n",
    "preds = pre_preds.T>0\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edd504-a328-40a7-8842-aea8f218d5c8",
   "metadata": {},
   "source": [
    "~~#TODO: Original nb had dual =True, check why~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e4776ca-3fd2-46d5-a2b4-b4d02ebe15b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 75132)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe774cb-f722-45b8-998d-8a8904ade12d",
   "metadata": {},
   "source": [
    "Setting *__dual=True__* because here number of features > number of samples. Setting this to True provides a lower bound to the solution of the primal (minimization) problem. Sklean recommends to set *__dual=False__* when n_samples > n_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "847da581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadil/miniconda3/envs/env_fastaai1/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.83268"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True, solver='liblinear')\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23d22d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aadil/miniconda3/envs/env_fastaai1/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8552"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=1e8, dual=True, solver='liblinear')\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecbabe-0a4a-4645-9071-5053513c38a9",
   "metadata": {},
   "source": [
    "~~#TODO: In original notebook, this is higher, `0.84x`~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38197965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84872"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True, solver='liblinear')\n",
    "m.fit(x, y)\n",
    "preds = m.predict(val_term_doc)\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13dad9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88404"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True, solver='liblinear')\n",
    "m.fit(trn_term_doc.sign(), y)\n",
    "preds = m.predict(val_term_doc.sign())\n",
    "(preds==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98aaa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n",
    "trn_term_doc = veczr.fit_transform(trn)\n",
    "val_term_doc = veczr.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2963dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27d5ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = veczr.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2018f753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[200000:200005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb4616de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=trn_y\n",
    "x=trn_term_doc.sign()\n",
    "val_x = val_term_doc.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9365590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(pr(1) / pr(0))\n",
    "b = np.log((y==1).mean() / (y==0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "daa52d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LogisticRegression(C=0.1, dual=True, solver='liblinear')\n",
    "m.fit(x, y);\n",
    "\n",
    "preds = m.predict(val_x)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0768174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 800000),\n",
       " matrix([[-0.05468386, -0.16100472, -0.24783616, ...,  1.09861229,\n",
       "          -0.69314718, -0.69314718]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c19951b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.94678442, 0.85128806, 0.7804878 , ..., 3.        , 0.5       ,\n",
       "         0.5       ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97d487dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91768"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nb = x.multiply(r)\n",
    "m = LogisticRegression(C=0.1, dual=True, solver='liblinear')\n",
    "m.fit(x_nb, y);\n",
    "\n",
    "val_x_nb = val_x.multiply(r)\n",
    "preds = m.predict(val_x_nb)\n",
    "(preds.T==val_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "63fa106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1230fc",
   "metadata": {},
   "source": [
    "~~# Note: Need to make things work after this~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48208496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how we get a model from a bag of words\n",
    "md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca0dab7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a66c462b974e3eaa7b73a405ff1f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1661840568476694851bf8a171e6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   \n",
      "    0      0.022909   0.120032   0.91632   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1200320093870163, 0.9163199999809265]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = md.dotprod_nb_learner()\n",
    "learner.fit(0.02, 1, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ee8a469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f054a8e0b00544b4a3f3eb709c2fc10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   \n",
      "    0      0.02172    0.113879   0.92112   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cb337aedff46b7b620fea64d9cd9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      0.011636   0.11232    0.92068   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11231955892562866, 0.920680000038147]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39221f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d67dc43a1542a1b2b9a79c01361428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   \n",
      "    0      0.016484   0.111565   0.92152   \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19aea79234d4601adbb691f7342b418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1      0.00962    0.11013    0.92132   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11012963932037353, 0.92132]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
